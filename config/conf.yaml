# This config file can be used for both run_exp.py and tune_alpha.py scripts 

deploy: False # Store run in wandb in True
tag: scratch  # Tag is used to determine logging-directory
seed: 0
random_reps: True
device: cuda:1

is_multihead: False                         # toggle between multi-head and single-head
ptw: False                                  # use pretrained weights
ptw_path: pretrained_imagenet_1_wrn10_2.pt  # sepcifcy the pretrained weights
eval_at_epoch: False 
reps: 1                                     # number of replicates

loss:
  alpha: 0.5                          # Weighted loss function
  tune_alpha: False          
  use_opt_alpha: False                # use tuned optimal alpha values
  tune_alpha_tag: 17_tune/cifar10     # the wandb log tag containing the optimal alpha values
  m_n_list: [0,1,2,3,4,5,10,20]       # the m_n list (required only if tune_alpha or use_opt_alpha is True)
  group_task_loss: False              # Avg losses based on task grouping

task:
  dataset: domainnet                                  # dataset
  target_env: real                                    # only for rotated CIFAR (angle), MNIST exps (angle), blurred CIFAR (sigma), PACS (env), OfficeHome (env)
  ood_env: quick                                      # only for rotated CIFAR (angle), MNIST exps (angle), blurred CIFAR (sigma), PACS (env), OfficeHome (env)
  task_map: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]  # task map (default: CIFAR-10 sub tasks)
  target: 0                                           # target sub task (only for CIFAR-10), for other datasets leave as 0
  ood: [0]                                            # OOD sub task (only for CIFAR-10), for other datasets leave as [0]
  augment: False
  n: 10                                               # Samples per label
  m_n: 1                                              # Samples of OOD per label
  beta: "unbiased"                                    # either float or string "unbiased" to use n/n+m
  custom_sampler: False
  

hp: 
  epochs: 150   # num epochs
  bs: 32        # batch size (for large datasets: 128, for small datasets: 16 or 32)
  lr: 0.01      # learning rate

net: wrn16_4    # network archiecture        


# Nested configs. Disable hydra logging
defaults:
  - _self_
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled
  - override hydra/launcher: joblib

# Disable hydra directory structure
hydra:
  output_subdir: Null
  run:
    dir: .

  sweep:
    dir: .
    subdir: .

  launcher:
    n_jobs: 10
